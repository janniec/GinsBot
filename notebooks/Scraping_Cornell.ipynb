{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitup(l, splitters):\n",
    "    current = []\n",
    "    for item in l:\n",
    "        if item in splitters:\n",
    "            yield current\n",
    "            current = []\n",
    "        else:\n",
    "            current.append(item)\n",
    "    yield current\n",
    "    # generator object splitup\n",
    "def sublist(l, splitters):\n",
    "    return [subl for subl in splitup(l, splitters) if subl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_links(judge):    \n",
    "    t0 = time.time()\n",
    "    response_delay = time.time() - t0\n",
    "    time.sleep(10*response_delay)  # wait 10x longer than it took them to respond\n",
    "    \n",
    "    website = 'https://www.law.cornell.edu/supct/justices/%s.dec.html'\n",
    "    url = website % judge\n",
    "    header = {'User-Agent': \n",
    "              'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36'}\n",
    "    soup =BeautifulSoup(requests.get(url, headers = header).text, \"lxml\")\n",
    "    \n",
    "    all_links =[]\n",
    "    for link in soup.find_all('ul')[9].find_all('a'):\n",
    "        try:\n",
    "            all_links.append(link['name']) \n",
    "        except:\n",
    "            try:\n",
    "                writing = {}\n",
    "                writing['name'] = link.text.strip()\n",
    "                writing['cite'] = link.next_sibling.strip().replace(',', '').strip()\n",
    "                writing['link'] = link['href']\n",
    "                all_links.append(writing)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    splits =[]\n",
    "    for link in soup.find_all('ul')[9].find_all('a', href=False):\n",
    "        splits.append(str(link['name'])) \n",
    "    \n",
    "    sublists_links = sublist(all_links, splits)\n",
    "    del soup\n",
    "    \n",
    "    return sublists_links  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_writings(judge):\n",
    "    sublists = all_links(judge)\n",
    "    subparts = {0: 'opinion', 1: 'concurrence', 2: 'dissent', 3: 'cdinpart'}\n",
    "    \n",
    "    for n in range(0, 4): #change it back to 4\n",
    "        for writing in sublists[n]:       \n",
    "            t0 = time.time()\n",
    "            response_delay = time.time() - t0\n",
    "            time.sleep(10*response_delay)  # wait 10x longer than it took them to respond\n",
    "    \n",
    "            website = 'https://www.law.cornell.edu/%s'\n",
    "            url = website % writing['link']\n",
    "            header = {'User-Agent': \n",
    "                  'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36'}\n",
    "            soup =BeautifulSoup(requests.get(url, headers = header).text, \"lxml\")\n",
    "            \n",
    "            writing['type'] = subparts[n] # added this after scraping Ginsburg, TEST\n",
    "        \n",
    "            try: # organized\n",
    "                writing['text'] = str(soup.find_all('div', {'class' : subparts[n]})[0].text.strip()).replace('\\n', ' ').replace('\\t', '').replace('TOP', '')\n",
    "            except: # disorganized\n",
    "#                 pass\n",
    "                try:\n",
    "                    writing['text'] = dis_organized(soup)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            print(writing['link'])\n",
    "            del soup\n",
    "            \n",
    "    sub_list = [j for i in sublists for j in i] # added this after scraping Ginsburg, TEST\n",
    "    return sub_list # added this after scraping Ginsburg, TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dis_organized(soup):\n",
    "    dirty = []\n",
    "    for paragraphs in soup.find_all('div', {'id': 'content'})[0].find_all('p'):\n",
    "        dirty.append(str(paragraphs.text.strip().replace('\\n', ' ').replace('\\t', '')))\n",
    "    keepers = judges_part(dirty, 'TOP')  \n",
    "    for paragraphs in keepers:\n",
    "        paragraphs = str(paragraphs).replace(\"\\'\",\"\").replace('\\\\', '')\n",
    "    return keepers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def judges_part(l, splitters):\n",
    "    all_opin = sublist(l, splitters)\n",
    "    sub_parts = []\n",
    "    for n in range(len(all_opin)):\n",
    "        if str(judge).upper() in all_opin[n][0]: #<---------- judge VARIABLE SET?\n",
    "            #NEXT STEP: if other judges names appear before judge name, skip.\n",
    "            sub_parts.append(all_opin[n])\n",
    "        else:\n",
    "            pass\n",
    "    parts = [j for i in sub_parts for j in i]\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_counter(list_o_dict): #Ginsbot[0]\n",
    "    count = 0\n",
    "    for writing in list_o_dict:\n",
    "        try:\n",
    "            if writing['text']:\n",
    "                count += 1\n",
    "        except:\n",
    "            pass\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def into_mongo(scrapings, judge):\n",
    "    client = pymongo.MongoClient()\n",
    "    db = client.supremes \n",
    "    db.create_collection(judge).insert(scrapings) # judge = \"ginsburg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Scrape into Mongo Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_into_mongo(judge):\n",
    "    judge = judge\n",
    "    scrapings = all_writings(judge)\n",
    "    into_mongo(scrapings, judge)\n",
    "    print(judge, \"is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrape_into_mongo('scalia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrape_into_mongo('ginsburg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
